import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
import os
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2, EfficientNetB0
import warnings
import json
import time
from pathlib import Path
import logging
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import cv2
from PIL import Image

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# T·∫Øt warnings kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class EnhancedPhoneDetectionCNN:
    def __init__(self, img_height=224, img_width=224, use_pretrained=True):
        self.img_height = img_height
        self.img_width = img_width
        self.model = None
        self.class_names = ['no_phone', 'using_phone']
        self.training_config = {}
        self.history = None
        self.use_pretrained = use_pretrained
        
        # Ki·ªÉm tra GPU
        self._check_gpu()
        
        # Thi·∫øt l·∫≠p memory growth cho GPU
        self._setup_gpu_memory()
    
    def _check_gpu(self):
        """Ki·ªÉm tra v√† thi·∫øt l·∫≠p GPU"""
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            logger.info(f"üéÆ T√¨m th·∫•y {len(gpus)} GPU(s)")
            for gpu in gpus:
                logger.info(f"   - {gpu}")
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y GPU, s·ª≠ d·ª•ng CPU")
    
    def _setup_gpu_memory(self):
        """Thi·∫øt l·∫≠p GPU memory growth ƒë·ªÉ tr√°nh l·ªói OOM"""
        try:
            gpus = tf.config.experimental.list_physical_devices('GPU')
            if gpus:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                logger.info("‚úÖ ƒê√£ thi·∫øt l·∫≠p GPU memory growth")
        except RuntimeError as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ thi·∫øt l·∫≠p GPU memory growth: {e}")
    
    def _normalize_path(self, path):
        """Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n ƒë·ªÉ tr√°nh l·ªói backslash tr√™n Windows"""
        if path:
            return str(Path(path).resolve())
        return path
    
    def _validate_data_structure(self, data_dir):
        """Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu v·ªõi x·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n c·∫£i thi·ªán"""
        data_dir = self._normalize_path(data_dir)
        
        if not os.path.exists(data_dir):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {data_dir}")
        
        class_counts = {}
        total_images = 0
        valid_files = []
        
        for class_name in self.class_names:
            class_path = os.path.join(data_dir, class_name)
            if not os.path.exists(class_path):
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c class: {class_path}")
                class_counts[class_name] = 0
                continue
            
            # ƒê·∫øm s·ªë ·∫£nh trong class v√† ki·ªÉm tra file
            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']
            class_files = []
            
            for root, dirs, files in os.walk(class_path):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in image_extensions):
                        full_path = os.path.join(root, file)
                        # Ki·ªÉm tra file c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng
                        try:
                            if os.path.getsize(full_path) > 0:  # File kh√¥ng r·ªóng
                                # Th√™m validation cho ·∫£nh
                                if self._is_valid_image(full_path):
                                    class_files.append(full_path)
                                    valid_files.append(full_path)
                        except (OSError, IOError) as e:
                            logger.warning(f"‚ö†Ô∏è  File l·ªói: {full_path} - {e}")
            
            class_counts[class_name] = len(class_files)
            total_images += len(class_files)
            
            logger.info(f"   {class_name}: {len(class_files)} ·∫£nh")
        
        if total_images == 0:
            raise ValueError(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh h·ª£p l·ªá n√†o trong {data_dir}")
        
        # Ki·ªÉm tra class imbalance
        if len(set(class_counts.values())) > 1:
            ratio = max(class_counts.values()) / max(min(class_counts.values()), 1)
            if ratio > 3:
                logger.warning(f"‚ö†Ô∏è  D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng (t·ª∑ l·ªá: {ratio:.1f}:1)")
                logger.info("üí° Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng class weights ho·∫∑c augmentation")
        
        return class_counts, total_images, valid_files
    
    def _is_valid_image(self, image_path):
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa ·∫£nh"""
        try:
            with Image.open(image_path) as img:
                img.verify()
            return True
        except Exception:
            return False
    
    def build_model(self, num_classes=2, dropout_rate=0.3):
        """T·∫°o model v·ªõi Transfer Learning ho·∫∑c CNN custom t√πy ch·ªçn"""
        try:
            if self.use_pretrained:
                # S·ª≠ d·ª•ng Transfer Learning v·ªõi MobileNetV2
                logger.info("üîÑ S·ª≠ d·ª•ng Transfer Learning v·ªõi MobileNetV2...")
                
                # Base model
                base_model = MobileNetV2(
                    input_shape=(self.img_height, self.img_width, 3),
                    include_top=False,
                    weights='imagenet'
                )
                
                # Freeze base model layers
                base_model.trainable = False
                
                # Add custom head
                model = models.Sequential([
                    layers.Input(shape=(self.img_height, self.img_width, 3)),
                    layers.Rescaling(1./255),
                    
                    # Preprocessing layers for better augmentation
                    layers.RandomFlip("horizontal"),
                    layers.RandomRotation(0.1),
                    layers.RandomZoom(0.1),
                    layers.RandomContrast(0.1),
                    layers.RandomBrightness(0.1),
                    
                    base_model,
                    layers.GlobalAveragePooling2D(),
                    layers.BatchNormalization(),
                    layers.Dropout(dropout_rate),
                    layers.Dense(128, activation='relu'),
                    layers.BatchNormalization(),
                    layers.Dropout(dropout_rate * 0.5),
                    layers.Dense(64, activation='relu'),
                    layers.Dropout(dropout_rate * 0.3),
                    layers.Dense(num_classes, activation='softmax')
                ])
                
            else:
                # Custom CNN v·ªõi ki·∫øn tr√∫c t·ªëi ∆∞u h∆°n
                logger.info("üèóÔ∏è X√¢y d·ª±ng Custom CNN...")
                
                model = models.Sequential([
                    layers.Input(shape=(self.img_height, self.img_width, 3)),
                    layers.Rescaling(1./255),
                    
                    # Augmentation layers
                    layers.RandomFlip("horizontal"),
                    layers.RandomRotation(0.1),
                    layers.RandomZoom(0.1),
                    layers.RandomContrast(0.1),
                    layers.RandomBrightness(0.1),
                    
                    # Block 1
                    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
                    layers.BatchNormalization(),
                    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
                    layers.MaxPooling2D((2, 2)),
                    layers.Dropout(0.25),
                    
                    # Block 2
                    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
                    layers.BatchNormalization(),
                    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
                    layers.MaxPooling2D((2, 2)),
                    layers.Dropout(0.25),
                    
                    # Block 3
                    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
                    layers.BatchNormalization(),
                    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
                    layers.MaxPooling2D((2, 2)),
                    layers.Dropout(0.25),
                    
                    # Block 4
                    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
                    layers.BatchNormalization(),
                    layers.GlobalAveragePooling2D(),
                    
                    # Classification head
                    layers.Dense(512, activation='relu'),
                    layers.BatchNormalization(),
                    layers.Dropout(dropout_rate),
                    layers.Dense(256, activation='relu'),
                    layers.BatchNormalization(),
                    layers.Dropout(dropout_rate * 0.5),
                    layers.Dense(num_classes, activation='softmax')
                ]
                )
            
            self.model = model
            logger.info(f"‚úÖ Model t·∫°o th√†nh c√¥ng v·ªõi {model.count_params():,} parameters")
            
            # In model summary
            model.summary()
            
            return model
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o model: {e}")
            raise
    
    def create_data_generators(self, train_dir, val_dir=None, batch_size=32, 
                             validation_split=0.2):
        """T·∫°o data generators v·ªõi augmentation m·∫°nh h∆°n"""
        try:
            # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n
            train_dir = self._normalize_path(train_dir)
            if val_dir:
                val_dir = self._normalize_path(val_dir)
            
            # Validate d·ªØ li·ªáu
            logger.info("üìä Ki·ªÉm tra d·ªØ li·ªáu training...")
            train_counts, train_total, valid_files = self._validate_data_structure(train_dir)
            
            # T·∫°o validation t·ª´ train n·∫øu kh√¥ng c√≥ val_dir
            if val_dir is None or not os.path.exists(val_dir):
                logger.info(f"üìÑ T·∫°o validation split t·ª´ training data ({validation_split*100}%)")
                
                # Training generator v·ªõi augmentation nh·∫π h∆°n
                train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    validation_split=validation_split,
                    rotation_range=8,           # gi·∫£m t·ª´ 20
                    width_shift_range=0.08,     # gi·∫£m t·ª´ 0.2
                    height_shift_range=0.08,    # gi·∫£m t·ª´ 0.2
                    shear_range=0.05,           # gi·∫£m t·ª´ 0.15
                    zoom_range=0.08,            # gi·∫£m t·ª´ 0.2
                    horizontal_flip=True,
                    brightness_range=[0.9, 1.1],# gi·∫£m bi√™n ƒë·ªô
                    channel_shift_range=0.05,   # gi·∫£m t·ª´ 0.1
                    fill_mode='nearest'
                )
                
                # Validation generator kh√¥ng augmentation
                val_datagen = ImageDataGenerator(
                    rescale=1./255,
                    validation_split=validation_split
                )
                
                train_generator = train_datagen.flow_from_directory(
                    train_dir,
                    target_size=(self.img_height, self.img_width),
                    batch_size=batch_size,
                    class_mode='categorical',
                    classes=self.class_names,
                    subset='training',
                    shuffle=True,
                    seed=42,
                    interpolation='bilinear'
                )
                
                val_generator = val_datagen.flow_from_directory(
                    train_dir,
                    target_size=(self.img_height, self.img_width),
                    batch_size=batch_size,
                    class_mode='categorical',
                    classes=self.class_names,
                    subset='validation',
                    shuffle=False,
                    seed=42,
                    interpolation='bilinear'
                )
                
            else:
                logger.info("üìä Ki·ªÉm tra d·ªØ li·ªáu validation...")
                val_counts, val_total, _ = self._validate_data_structure(val_dir)
                
                # Separate train v√† validation directories
                train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=8,
                    width_shift_range=0.08,
                    height_shift_range=0.08,
                    shear_range=0.05,
                    zoom_range=0.08,
                    horizontal_flip=True,
                    brightness_range=[0.9, 1.1],
                    channel_shift_range=0.05,
                    fill_mode='nearest'
                )
                
                val_datagen = ImageDataGenerator(rescale=1./255)
                
                train_generator = train_datagen.flow_from_directory(
                    train_dir,
                    target_size=(self.img_height, self.img_width),
                    batch_size=batch_size,
                    class_mode='categorical',
                    classes=self.class_names,
                    shuffle=True,
                    seed=42,
                    interpolation='bilinear'
                )
                
                val_generator = val_datagen.flow_from_directory(
                    val_dir,
                    target_size=(self.img_height, self.img_width),
                    batch_size=batch_size,
                    class_mode='categorical',
                    classes=self.class_names,
                    shuffle=False,
                    seed=42,
                    interpolation='bilinear'
                )
            
            # Ki·ªÉm tra batch ƒë·∫ßu ti√™n
            self._test_generators(train_generator, val_generator)
            
            return train_generator, val_generator
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o data generators: {e}")
            raise
    
    def _test_generators(self, train_gen, val_gen):
        """Test generators ƒë·ªÉ ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng"""
        try:
            logger.info("üß™ Test data generators...")
            
            # Test train generator
            train_batch = next(train_gen)
            logger.info(f"   Train batch shape: {train_batch[0].shape}")
            logger.info(f"   Train labels shape: {train_batch[1].shape}")
            logger.info(f"   Train data range: [{train_batch[0].min():.3f}, {train_batch[0].max():.3f}]")
            
            # Test validation generator
            if val_gen:
                val_batch = next(val_gen)
                logger.info(f"   Val batch shape: {val_batch[0].shape}")
                logger.info(f"   Val labels shape: {val_batch[1].shape}")
                logger.info(f"   Val data range: [{val_batch[0].min():.3f}, {val_batch[0].max():.3f}]")
            
            logger.info("‚úÖ Data generators ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi test generators: {e}")
            raise
    
    def calculate_class_weights(self, train_generator):
        """T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω imbalanced data"""
        try:
            # L·∫•y labels t·ª´ generator
            labels = train_generator.classes
            class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(labels),
                y=labels
            )
            
            class_weight_dict = dict(enumerate(class_weights))
            logger.info(f"üìä Class weights: {class_weight_dict}")
            
            return class_weight_dict
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t√≠nh class weights: {e}")
            return None
    
    def train(self, train_generator, val_generator=None, epochs=50, 
              learning_rate=0.001, save_path='models/enhanced_phone_detection_model.keras',
              use_class_weights=True, patience=15, fine_tune_epochs=10):
        """Training v·ªõi Transfer Learning v√† Fine-tuning"""
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu training...")
            
            # T·∫°o th∆∞ m·ª•c l∆∞u model
            save_path = self._normalize_path(save_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # L∆∞u config
            self.training_config = {
                'epochs': epochs,
                'learning_rate': learning_rate,
                'batch_size': train_generator.batch_size,
                'image_size': (self.img_height, self.img_width),
                'total_train_samples': train_generator.samples,
                'total_val_samples': val_generator.samples if val_generator else 0,
                'use_class_weights': use_class_weights,
                'use_pretrained': self.use_pretrained,
                'fine_tune_epochs': fine_tune_epochs
            }
            
            # T√≠nh class weights
            class_weights = None
            if use_class_weights:
                class_weights = self.calculate_class_weights(train_generator)
            
            # Phase 1: Train with frozen base model
            logger.info("üìö Phase 1: Training v·ªõi base model frozen...")
            
            optimizer = keras.optimizers.Adam(
                learning_rate=learning_rate,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8
            )
            
            self.model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',
                metrics=[
                    'accuracy',
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall')
                ]
            )
            
            # Callbacks
            callbacks = self._create_callbacks(save_path, val_generator, patience)
            
            # T√≠nh steps per epoch
            steps_per_epoch = max(1, train_generator.samples // train_generator.batch_size)
            validation_steps = None
            if val_generator:
                validation_steps = max(1, val_generator.samples // val_generator.batch_size)
            
            logger.info(f"üìä Training info:")
            logger.info(f"   Steps per epoch: {steps_per_epoch}")
            logger.info(f"   Validation steps: {validation_steps}")
            logger.info(f"   Total parameters: {self.model.count_params():,}")
            logger.info(f"   Learning rate: {learning_rate}")
            
            # Phase 1 Training
            start_time = time.time()
            
            history1 = self.model.fit(
                train_generator,
                steps_per_epoch=steps_per_epoch,
                epochs=epochs,
                validation_data=val_generator,
                validation_steps=validation_steps,
                callbacks=callbacks,
                class_weight=class_weights,
                verbose=1
            )
            
            # Phase 2: Fine-tuning (n·∫øu s·ª≠ d·ª•ng pretrained model)
            if self.use_pretrained and fine_tune_epochs > 0:
                logger.info("üîß Phase 2: Fine-tuning...")
                
                # Unfreeze top layers c·ªßa base model
                base_model = self.model.layers[6]  # MobileNetV2 layer
                base_model.trainable = True
                
                # Freeze bottom layers, ch·ªâ train top layers
                for layer in base_model.layers[:-20]:
                    layer.trainable = False
                
                # Compile v·ªõi learning rate th·∫•p h∆°n
                self.model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate/10),
                    loss='categorical_crossentropy',
                    metrics=[
                        'accuracy',
                        tf.keras.metrics.Precision(name='precision'),
                        tf.keras.metrics.Recall(name='recall'),
                    ]
                )
                
                # Fine-tuning callbacks
                fine_tune_callbacks = self._create_callbacks(
                    save_path.replace('.keras', '_fine_tuned.keras'), 
                    val_generator, 
                    patience//2
                )
                
                history2 = self.model.fit(
                    train_generator,
                    steps_per_epoch=steps_per_epoch,
                    epochs=fine_tune_epochs,
                    validation_data=val_generator,
                    validation_steps=validation_steps,
                    callbacks=fine_tune_callbacks,
                    class_weight=class_weights,
                    verbose=1
                )
                
                # Combine histories
                history = self._combine_histories(history1, history2)
            else:
                history = history1
            
            training_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Training ho√†n th√†nh trong {training_time/60:.1f} ph√∫t")
            
            # L∆∞u history v√† config
            self.history = history
            self._save_training_info(save_path, history, training_time)
            
            # V·∫Ω k·∫øt qu·∫£
            self.plot_training_history(history, save_path)
            
            return history
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è Training b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
            if self.model:
                interrupt_path = save_path.replace('.keras', '_interrupted.keras')
                self.model.save(interrupt_path)
                logger.info(f"üíæ Model ƒë√£ l∆∞u t·∫°i: {interrupt_path}")
            raise
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh training: {e}")
            raise
    
    def _combine_histories(self, history1, history2):
        """K·∫øt h·ª£p 2 history objects"""
        combined = type('History', (), {})()
        combined.history = {}
        
        for key in history1.history:
            combined.history[key] = history1.history[key] + history2.history[key]
        
        return combined
    
    def _create_callbacks(self, save_path, val_generator, patience):
        """T·∫°o callbacks c·∫£i thi·ªán"""
        callbacks = []
        
        # Early stopping
        monitor = 'val_loss' if val_generator else 'loss'
        early_stopping = keras.callbacks.EarlyStopping(
            monitor=monitor,
            patience=patience,
            restore_best_weights=True,
            verbose=1,
            mode='min',
            min_delta=0.0001
        )
        callbacks.append(early_stopping)
        
        # Model checkpoint
        checkpoint = keras.callbacks.ModelCheckpoint(
            save_path,
            monitor=monitor,
            save_best_only=True,
            save_weights_only=False,
            verbose=1,
            mode='min'
        )
        callbacks.append(checkpoint)
        
        # Reduce learning rate
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor=monitor,
            factor=0.5,
            patience=max(5, patience//3),
            min_lr=1e-8,
            verbose=1,
            mode='min',
            min_delta=0.0001
        )
        callbacks.append(reduce_lr)
        
        # CSV Logger
        csv_path = save_path.replace('.keras', '_training_log.csv')
        csv_logger = keras.callbacks.CSVLogger(csv_path, append=True)
        callbacks.append(csv_logger)
        
        return callbacks
    
    def _save_training_info(self, save_path, history, training_time):
        """L∆∞u th√¥ng tin training"""
        try:
            info_path = save_path.replace('.keras', '_info.json')
            
            # Chu·∫©n b·ªã th√¥ng tin
            training_info = {
                'config': self.training_config,
                'training_time_minutes': training_time / 60,
                'final_metrics': {
                    'train_loss': float(history.history['loss'][-1]),
                    'train_accuracy': float(history.history['accuracy'][-1]),
                },
                'best_metrics': {
                    'best_train_loss': float(min(history.history['loss'])),
                    'best_train_accuracy': float(max(history.history['accuracy'])),
                },
                'total_epochs': len(history.history['loss'])
            }
            
            # Th√™m validation metrics n·∫øu c√≥
            if 'val_loss' in history.history:
                training_info['final_metrics'].update({
                    'val_loss': float(history.history['val_loss'][-1]),
                    'val_accuracy': float(history.history['val_accuracy'][-1]),
                })
                training_info['best_metrics'].update({
                    'best_val_loss': float(min(history.history['val_loss'])),
                    'best_val_accuracy': float(max(history.history['val_accuracy'])),
                })
            
            # L∆∞u file
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(training_info, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìÑ Training info saved: {info_path}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u training info: {e}")
    
    def plot_training_history(self, history, save_path):
        """V·∫Ω bi·ªÉu ƒë·ªì training history c·∫£i thi·ªán"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # Loss plot
            axes[0,0].plot(history.history['loss'], label='Training Loss', linewidth=2)
            if 'val_loss' in history.history:
                axes[0,0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
            axes[0,0].set_title('Model Loss', fontsize=14, fontweight='bold')
            axes[0,0].set_xlabel('Epoch')
            axes[0,0].set_ylabel('Loss')
            axes[0,0].legend()
            axes[0,0].grid(True, alpha=0.3)
            
            # Accuracy plot
            axes[0,1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
            if 'val_accuracy' in history.history:
                axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
            axes[0,1].set_title('Model Accuracy', fontsize=14, fontweight='bold')
            axes[0,1].set_xlabel('Epoch')
            axes[0,1].set_ylabel('Accuracy')
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            
            # Precision plot
            if 'precision' in history.history:
                axes[1,0].plot(history.history['precision'], label='Training Precision', linewidth=2)
                if 'val_precision' in history.history:
                    axes[1,0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)
                axes[1,0].set_title('Model Precision', fontsize=14, fontweight='bold')
                axes[1,0].set_xlabel('Epoch')
                axes[1,0].set_ylabel('Precision')
                axes[1,0].legend()
                axes[1,0].grid(True, alpha=0.3)
            
            # F1 Score plot
            if 'f1_score' in history.history:
                axes[1,1].plot(history.history['f1_score'], label='Training F1 Score', linewidth=2)
                if 'val_f1_score' in history.history:
                    axes[1,1].plot(history.history['val_f1_score'], label='Validation F1 Score', linewidth=2)
                axes[1,1].set_title('Model F1 Score', fontsize=14, fontweight='bold')
                axes[1,1].set_xlabel('Epoch')
                axes[1,1].set_ylabel('F1 Score')
                axes[1,1].legend()
                axes[1,1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # L∆∞u bi·ªÉu ƒë·ªì
            plot_path = save_path.replace('.keras', '_training_plot.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            logger.info(f"üìà Training plot saved: {plot_path}")
            
            plt.show()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì: {e}")
    
    def evaluate_model(self, test_generator, save_results=True, save_path=None):
        """ƒê√°nh gi√° model chi ti·∫øt v·ªõi confusion matrix"""
        try:
            if self.model is None:
                raise ValueError("Model ch∆∞a ƒë∆∞·ª£c training ho·∫∑c load!")
            
            logger.info("üß™ ƒê√°nh gi√° model...")
            
            # Reset generator
            test_generator.reset()
            
            # Predict
            steps = max(1, test_generator.samples // test_generator.batch_size)
            predictions = self.model.predict(test_generator, steps=steps, verbose=1)
            
            # Get true labels
            true_labels = test_generator.classes[:len(predictions)]
            predicted_labels = np.argmax(predictions, axis=1)
            
            # Calculate metrics
            from sklearn.metrics import accuracy_score, precision_recall_fscore_support
            
            accuracy = accuracy_score(true_labels, predicted_labels)
            precision, recall, f1, support = precision_recall_fscore_support(
                true_labels, predicted_labels, average='weighted'
            )
            
            logger.info(f"üìä K·∫øt qu·∫£ ƒë√°nh gi√°:")
            logger.info(f"   Accuracy: {accuracy:.4f}")
            logger.info(f"   Precision: {precision:.4f}")
            logger.info(f"   Recall: {recall:.4f}")
            logger.info(f"   F1 Score: {f1:.4f}")
            
            # Confusion Matrix
            cm = confusion_matrix(true_labels, predicted_labels)
            
            if save_results and save_path:
                self._plot_confusion_matrix(cm, save_path)
                self._save_classification_report(true_labels, predicted_labels, save_path)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'confusion_matrix': cm,
                'predictions': predictions,
                'true_labels': true_labels
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë√°nh gi√° model: {e}")
            raise
    
    def _plot_confusion_matrix(self, cm, save_path):
        """V·∫Ω confusion matrix"""
        try:
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=self.class_names,
                       yticklabels=self.class_names)
            plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
            plt.xlabel('Predicted Label', fontsize=12)
            plt.ylabel('True Label', fontsize=12)
            
            cm_path = save_path.replace('.keras', '_confusion_matrix.png')
            plt.savefig(cm_path, dpi=300, bbox_inches='tight', facecolor='white')
            logger.info(f"üìä Confusion matrix saved: {cm_path}")
            plt.show()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ v·∫Ω confusion matrix: {e}")
    
    def _save_classification_report(self, true_labels, predicted_labels, save_path):
        """L∆∞u classification report"""
        try:
            report = classification_report(
                true_labels, predicted_labels,
                target_names=self.class_names,
                output_dict=True
            )
            
            report_path = save_path.replace('.keras', '_classification_report.json')
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìã Classification report saved: {report_path}")
            
            # In report ra console
            print("\n" + "="*50)
            print("CLASSIFICATION REPORT")
            print("="*50)
            print(classification_report(true_labels, predicted_labels, target_names=self.class_names))
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u classification report: {e}")
    
    def predict_single_image(self, image_path, show_result=True):
        """D·ª± ƒëo√°n cho m·ªôt ·∫£nh"""
        try:
            if self.model is None:
                raise ValueError("Model ch∆∞a ƒë∆∞·ª£c training ho·∫∑c load!")
            
            # Chu·∫©n h√≥a ƒë∆∞·ªùng d·∫´n
            image_path = self._normalize_path(image_path)
            
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y ·∫£nh: {image_path}")
            
            # Load v√† preprocess ·∫£nh
            img = keras.utils.load_img(
                image_path, 
                target_size=(self.img_height, self.img_width)
            )
            img_array = keras.utils.img_to_array(img)
            img_array = tf.expand_dims(img_array, 0)  # T·∫°o batch
            
            # Predict
            predictions = self.model.predict(img_array, verbose=0)
            confidence = np.max(predictions[0])
            predicted_class_idx = np.argmax(predictions[0])
            predicted_class = self.class_names[predicted_class_idx]
            
            result = {
                'predicted_class': predicted_class,
                'confidence': float(confidence),
                'all_predictions': {
                    self.class_names[i]: float(predictions[0][i]) 
                    for i in range(len(self.class_names))
                }
            }
            
            if show_result:
                self._show_prediction_result(image_path, img, result)
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi d·ª± ƒëo√°n ·∫£nh: {e}")
            raise
    
    def _show_prediction_result(self, image_path, img, result):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n"""
        try:
            plt.figure(figsize=(10, 6))
            
            # Hi·ªÉn th·ªã ·∫£nh
            plt.subplot(1, 2, 1)
            plt.imshow(img)
            plt.title(f"Input Image\n{os.path.basename(image_path)}", fontsize=12)
            plt.axis('off')
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            plt.subplot(1, 2, 2)
            classes = list(result['all_predictions'].keys())
            confidences = list(result['all_predictions'].values())
            
            colors = ['red' if c == result['predicted_class'] else 'blue' for c in classes]
            bars = plt.bar(classes, confidences, color=colors, alpha=0.7)
            
            plt.title(f"Prediction Results\nPredicted: {result['predicted_class']}\nConfidence: {result['confidence']:.3f}", fontsize=12)
            plt.ylabel('Confidence')
            plt.ylim(0, 1)
            
            # Th√™m text tr√™n bars
            for bar, conf in zip(bars, confidences):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            plt.show()
            
            # In k·∫øt qu·∫£
            print(f"\nüéØ K·∫øt qu·∫£ d·ª± ƒëo√°n cho: {os.path.basename(image_path)}")
            print(f"   Predicted Class: {result['predicted_class']}")
            print(f"   Confidence: {result['confidence']:.4f}")
            print(f"   All Predictions:")
            for class_name, conf in result['all_predictions'].items():
                print(f"     {class_name}: {conf:.4f}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£: {e}")
    
    def predict_batch_images(self, image_folder, output_csv=None, min_confidence=0.5):
        """D·ª± ƒëo√°n batch ·∫£nh"""
        try:
            if self.model is None:
                raise ValueError("Model ch∆∞a ƒë∆∞·ª£c training ho·∫∑c load!")
            
            image_folder = self._normalize_path(image_folder)
            
            if not os.path.exists(image_folder):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {image_folder}")
            
            # T√¨m t·∫•t c·∫£ ·∫£nh
            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']
            image_files = []
            
            for root, dirs, files in os.walk(image_folder):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in image_extensions):
                        image_files.append(os.path.join(root, file))
            
            if not image_files:
                raise ValueError(f"Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o trong {image_folder}")
            
            logger.info(f"üîç ƒêang d·ª± ƒëo√°n {len(image_files)} ·∫£nh...")
            
            results = []
            failed_images = []
            
            for i, image_path in enumerate(image_files):
                try:
                    result = self.predict_single_image(image_path, show_result=False)
                    result['image_path'] = image_path
                    result['image_name'] = os.path.basename(image_path)
                    result['high_confidence'] = result['confidence'] >= min_confidence
                    results.append(result)
                    
                    if (i + 1) % 50 == 0:
                        logger.info(f"   ƒê√£ x·ª≠ l√Ω {i + 1}/{len(image_files)} ·∫£nh")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  L·ªói v·ªõi ·∫£nh {image_path}: {e}")
                    failed_images.append(image_path)
            
            # Th·ªëng k√™ k·∫øt qu·∫£
            total_processed = len(results)
            high_confidence_count = sum(1 for r in results if r['high_confidence'])
            class_counts = {}
            for class_name in self.class_names:
                class_counts[class_name] = sum(1 for r in results if r['predicted_class'] == class_name)
            
            logger.info(f"üìä K·∫øt qu·∫£ batch prediction:")
            logger.info(f"   T·ªïng ·∫£nh x·ª≠ l√Ω: {total_processed}")
            logger.info(f"   ·∫¢nh failed: {len(failed_images)}")
            logger.info(f"   ·∫¢nh high confidence (>={min_confidence}): {high_confidence_count}")
            logger.info(f"   Ph√¢n b·ªë class:")
            for class_name, count in class_counts.items():
                logger.info(f"     {class_name}: {count} ·∫£nh")
            
            # L∆∞u CSV n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
            if output_csv and results:
                self._save_batch_results_to_csv(results, output_csv)
            
            return {
                'results': results,
                'failed_images': failed_images,
                'statistics': {
                    'total_processed': total_processed,
                    'total_failed': len(failed_images),
                    'high_confidence_count': high_confidence_count,
                    'class_distribution': class_counts
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi d·ª± ƒëo√°n batch: {e}")
            raise
    
    def _save_batch_results_to_csv(self, results, output_csv):
        """L∆∞u k·∫øt qu·∫£ batch v√†o CSV"""
        try:
            import pandas as pd
            
            # Chu·∫©n b·ªã data cho CSV
            csv_data = []
            for result in results:
                row = {
                    'image_path': result['image_path'],
                    'image_name': result['image_name'],
                    'predicted_class': result['predicted_class'],
                    'confidence': result['confidence'],
                    'high_confidence': result['high_confidence']
                }
                
                # Th√™m confidence cho t·ª´ng class
                for class_name, conf in result['all_predictions'].items():
                    row[f'confidence_{class_name}'] = conf
                
                csv_data.append(row)
            
            # T·∫°o DataFrame v√† l∆∞u
            df = pd.DataFrame(csv_data)
            df.to_csv(output_csv, index=False, encoding='utf-8')
            
            logger.info(f"üíæ Batch results saved to: {output_csv}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u CSV: {e}")
    
    def load_model(self, model_path):
        """Load model ƒë√£ training"""
        try:
            model_path = self._normalize_path(model_path)
            
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model: {model_path}")
            
            logger.info(f"üìÇ Loading model t·ª´: {model_path}")
            
            # Load model
            self.model = keras.models.load_model(model_path)
            
            # Load training info n·∫øu c√≥
            info_path = model_path.replace('.keras', '_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r', encoding='utf-8') as f:
                    self.training_config = json.load(f)
                logger.info("üìÑ ƒê√£ load training config")
            
            logger.info("‚úÖ Model loaded th√†nh c√¥ng!")
            
            # In th√¥ng tin model
            logger.info(f"üìä Model info:")
            logger.info(f"   Total parameters: {self.model.count_params():,}")
            logger.info(f"   Input shape: {self.model.input_shape}")
            logger.info(f"   Output shape: {self.model.output_shape}")
            
            return self.model
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load model: {e}")
            raise
    
    def save_model(self, save_path):
        """L∆∞u model"""
        try:
            if self.model is None:
                raise ValueError("Kh√¥ng c√≥ model ƒë·ªÉ l∆∞u!")
            
            save_path = self._normalize_path(save_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # L∆∞u model
            self.model.save(save_path)
            logger.info(f"üíæ Model ƒë√£ l∆∞u t·∫°i: {save_path}")
            
            # L∆∞u training config n·∫øu c√≥
            if self.training_config:
                info_path = save_path.replace('.keras', '_info.json')
                with open(info_path, 'w', encoding='utf-8') as f:
                    json.dump(self.training_config, f, indent=2, ensure_ascii=False)
                logger.info(f"üìÑ Training config ƒë√£ l∆∞u t·∫°i: {info_path}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u model: {e}")
            raise


# V√≠ d·ª• s·ª≠ d·ª•ng
def main():
    """V√≠ d·ª• s·ª≠ d·ª•ng Enhanced Phone Detection CNN"""
    try:
        # Kh·ªüi t·∫°o model
        print("üöÄ Kh·ªüi t·∫°o Enhanced Phone Detection CNN...")
        phone_detector = EnhancedPhoneDetectionCNN(
            img_height=224,
            img_width=224,
            use_pretrained=True  # S·ª≠ d·ª•ng Transfer Learning
        )
        
        # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu (thay ƒë·ªïi theo ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø c·ªßa b·∫°n)
        train_data_dir = "data/train"  # Th∆∞ m·ª•c ch·ª©a 2 folder: no_phone v√† using_phone
        test_data_dir = "data/test"    # T∆∞∆°ng t·ª± cho test
        
        # T·∫°o model
        print("\nüèóÔ∏è X√¢y d·ª±ng model...")
        model = phone_detector.build_model(
            num_classes=2,
            dropout_rate=0.3
        )
        
        # T·∫°o data generators
        print("\nüìä T·∫°o data generators...")
        train_gen, val_gen = phone_detector.create_data_generators(
            train_dir=train_data_dir,
            val_dir=None,  # S·∫Ω t·ª± ƒë·ªông split t·ª´ train
            batch_size=32,
            validation_split=0.2
        )
        
        # Training
        print("\nüöÄ B·∫Øt ƒë·∫ßu training...")
        history = phone_detector.train(
            train_generator=train_gen,
            val_generator=val_gen,
            epochs=100,  # tƒÉng s·ªë epoch
            learning_rate=0.0005,  # gi·∫£m learning rate
            save_path='models/enhanced_phone_detection_model.keras',
            use_class_weights=True,
            patience=20,  # tƒÉng patience cho early stopping
            fine_tune_epochs=20  # tƒÉng fine-tune epochs
        )
        
        # ƒê√°nh gi√° model
        if os.path.exists(test_data_dir):
            print("\nüß™ ƒê√°nh gi√° model tr√™n test set...")
            test_datagen = ImageDataGenerator(rescale=1./255)
            test_gen = test_datagen.flow_from_directory(
                test_data_dir,
                target_size=(224, 224),
                batch_size=32,
                class_mode='categorical',
                classes=phone_detector.class_names,
                shuffle=False
            )
            
            evaluation_results = phone_detector.evaluate_model(
                test_gen,
                save_results=True,
                save_path='models/enhanced_phone_detection_model.keras'
            )
            
            print(f"\nüìä Final Test Results:")
            print(f"   Accuracy: {evaluation_results['accuracy']:.4f}")
            print(f"   Precision: {evaluation_results['precision']:.4f}")
            print(f"   Recall: {evaluation_results['recall']:.4f}")
            print(f"   F1 Score: {evaluation_results['f1_score']:.4f}")
        
        # V√≠ d·ª• predict single image
        print("\nüéØ V√≠ d·ª• predict single image...")
        sample_image = "sample_images/test_image.jpg"  # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n
        if os.path.exists(sample_image):
            result = phone_detector.predict_single_image(
                sample_image,
                show_result=True
            )
            print(f"K·∫øt qu·∫£: {result['predicted_class']} (confidence: {result['confidence']:.4f})")
        
        # V√≠ d·ª• batch prediction
        print("\nüìÅ V√≠ d·ª• batch prediction...")
        batch_folder = "sample_images/"  # Th∆∞ m·ª•c ch·ª©a nhi·ªÅu ·∫£nh test
        if os.path.exists(batch_folder):
            batch_results = phone_detector.predict_batch_images(
                batch_folder,
                output_csv="results/batch_predictions.csv",
                min_confidence=0.7
            )
            
            print(f"ƒê√£ x·ª≠ l√Ω {batch_results['statistics']['total_processed']} ·∫£nh")
            print(f"High confidence predictions: {batch_results['statistics']['high_confidence_count']}")
        
        print("\n‚úÖ Ho√†n th√†nh!")
        
    except FileNotFoundError as e:
        print(f"\n‚ùå L·ªói file/folder: {e}")
        print("üí° H∆∞·ªõng d·∫´n:")
        print("   1. T·∫°o th∆∞ m·ª•c data/train v·ªõi 2 subfolder: no_phone v√† using_phone")
        print("   2. Th√™m ·∫£nh training v√†o c√°c subfolder t∆∞∆°ng ·ª©ng")
        print("   3. (T√πy ch·ªçn) T·∫°o data/test v·ªõi c·∫•u tr√∫c t∆∞∆°ng t·ª± cho evaluation")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()